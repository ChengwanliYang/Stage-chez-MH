{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db827792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import dataiku\n",
    "import pandas as pd, numpy as np\n",
    "from dataiku import pandasutils as pdu\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS as stop_sp\n",
    "\n",
    "# Read recipe inputs\n",
    "trans_json = dataiku.Dataset(\"Trans_json\")\n",
    "trans_json_df = trans_json.get_dataframe()\n",
    "\n",
    "stopwords_fr = dataiku.Dataset(\"stopwords_fr_1\")\n",
    "stopwords_fr_df = stopwords_fr.get_dataframe()\n",
    "liste_stopword = list(stopwords_fr_df['line'])\n",
    "\n",
    "# Compute recipe outputs from inputs\n",
    "# TODO: Replace this part by your actual code that computes the output, as a Pandas dataframe\n",
    "# NB: DSS also supports other kinds of APIs for reading and writing data. Please see doc.\n",
    "\n",
    "token_insatis_df = trans_json_df # For this sample code, simply copy input to output\n",
    "\n",
    "\n",
    "# Write recipe outputs\n",
    "#token_insatis = dataiku.Dataset(\"token_insatis\")\n",
    "#token_insatis.write_with_schema(token_insatis_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7014c35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "trans = token_insatis_df['transcript_json.callData']\n",
    "liste_dic = []\n",
    "#new_df = pd.DataFrame(columns = ['NER','score','duree',\"nom\"])\n",
    "\n",
    "\n",
    "for i in range(len(trans)):\n",
    "    dic_inter = {}\n",
    "    id_conv = trans_json_df['metadata.reconciliation_id'][i]\n",
    "    dic_inter['nom'] = id_conv\n",
    "    one_trans = trans[i]\n",
    "    trans_dict = json.loads(one_trans)\n",
    "    liste_token_insu = []\n",
    "    \n",
    "    for l in trans_dict:\n",
    "        \n",
    "\n",
    "\n",
    "        word = l['words']\n",
    "        for m in word:\n",
    "            value = m['value']\n",
    "            matchObj = re.findall('\\[.+\\]', value)\n",
    "\n",
    "            if matchObj:\n",
    "                pass\n",
    "            \n",
    "            \n",
    "            else :\n",
    "                    \n",
    "                    \n",
    "                    score = float(m['score'])\n",
    "                    if score>= 0 and score < 0.7:\n",
    "                        \n",
    "                        value = m['value']\n",
    "                        \n",
    "                        \n",
    "                        liste_token_insu.append(value)\n",
    "                        \n",
    "    #print(liste_token_insu)   \n",
    "    dic_inter['token'] = liste_token_insu\n",
    "                    \n",
    "\n",
    "                    \n",
    "\n",
    "    liste_dic.append(dic_inter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d1fc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stop_nltk = list(stopwords.words('french'))\n",
    "liste_stop_sp = list(stop_sp)\n",
    "liste_stop_sp.append(\"euh\")\n",
    "liste_stop_sp.append(\"ben\")\n",
    "liste_stop_sp.append(\"hein\")\n",
    "liste_stop_sp.append(\"hum\")\n",
    "liste_stop_sp.append(\"hop\")\n",
    "liste_stop_sp.append(\"bah\")\n",
    "liste_stop_sp.append(\"c'est\")\n",
    "liste_stop_sp.append(\"ouais\")\n",
    "liste_stop_sp.append(\"j'ai\")\n",
    "\n",
    "liste_stop = set(stop_nltk+liste_stop_sp+liste_stopword)\n",
    "#print(liste_stop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1eefa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(liste_dic)-1):\n",
    "    conver = liste_dic[i]\n",
    "    token_insuff = conver['token']\n",
    "    if token_insuff == []:\n",
    "        liste_dic.remove(conver)\n",
    "#print(len(liste_dic))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb52aab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for conver in liste_dic:\n",
    "    token_insuff = conver['token']\n",
    "    #str_token = str(token_insuff)\n",
    "    #a = str_token.split(\"'\")\n",
    "    liste_token_net = [word for word in token_insuff if word not in liste_stop]\n",
    "    conver['token'] = set(liste_token_net)\n",
    "    '''\n",
    "    token_insu = conver['token']\n",
    "    for m in token_insu:\n",
    "        freq = token_insu.count(m)\n",
    "        print(m,freq)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d00027",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(liste_dic)\n",
    "#new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87f379d",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for conver in liste_dic:\n",
    "    token_insuff = conver['token']\n",
    "    for i in token_insuff:\n",
    "        l.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c7bdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_2 = pd.DataFrame(columns = [\"token\"])\n",
    "new_df_2[\"token\"] = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35942b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_insatis = dataiku.Dataset(\"token_insuff_net\")\n",
    "token_insatis.write_with_schema(new_df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df8146d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('fr_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef1150b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "doc = nlp(str(l))\n",
    "entities, labels, position_start, position_end = [], [], [], []\n",
    "\n",
    "for ent in doc.ents:\n",
    "    entities.append(ent)\n",
    "    labels.append(ent.label_)\n",
    "    position_start.append(ent.start_char)\n",
    "    position_end.append(ent.end_char)\n",
    "    \n",
    "df = pd.DataFrame({'Entities':entities,'Labels':labels,'Position_Start':position_start, 'Position_End':position_end})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ca3fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
