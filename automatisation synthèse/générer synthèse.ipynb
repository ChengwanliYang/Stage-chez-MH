{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0b539d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import dataiku\n",
    "import pandas as pd, numpy as np\n",
    "from dataiku import pandasutils as pdu\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "# Read recipe inputs\n",
    "latest_transcription = dataiku.Dataset(\"transcription_latest\")\n",
    "latest_transcription_df = latest_transcription.get_dataframe()\n",
    "\n",
    "conversations_diarisees = latest_transcription_df['text_diarise']\n",
    "liste_de_conversations_diarisees = list(conversations_diarisees)\n",
    "\n",
    "# Compute recipe outputs from inputs\n",
    "# TODO: Replace this part by your actual code that computes the output, as a Pandas dataframe\n",
    "# NB: DSS also supports other kinds of APIs for reading and writing data. Please see doc.\n",
    "\n",
    "#synthese_transcription_df = latest_transcription_df # For this sample code, simply copy input to output\n",
    "\n",
    "\n",
    "# Write recipe outputs\n",
    "#synthese_transcription = dataiku.Dataset(\"synthese_transcription\")\n",
    "#synthese_transcription.write_with_schema(synthese_transcription_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d384878",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nettoie_resume(texte):\n",
    "\n",
    "    l = []\n",
    "    for i in texte:\n",
    "        if \"s'il vous plaît\" in i:\n",
    "            pass\n",
    "        else:\n",
    "            l.append(i)\n",
    "\n",
    "    l2 = []\n",
    "    for i in l:\n",
    "        if \"bonjour que puis-je faire pour vous\" in i:\n",
    "            pass\n",
    "        else:\n",
    "            l2.append(i)\n",
    "\n",
    "\n",
    "\n",
    "    l3 = []\n",
    "    for i in l2:\n",
    "        if \"votre nom\" in i:\n",
    "            pass\n",
    "        else:\n",
    "            l3.append(i)\n",
    "\n",
    "    l4 = []\n",
    "    for i in l3:\n",
    "        if \"au revoir\" in i:\n",
    "            pass\n",
    "        else:\n",
    "            l4.append(i)\n",
    "\n",
    "    l5 = []\n",
    "    for i in l4:\n",
    "        if \"bonne journée\" in i:\n",
    "            pass\n",
    "        else:\n",
    "            l5.append(i)\n",
    "\n",
    "    l6 = []\n",
    "    for i in l5:\n",
    "        if \"adresse postale\" in i:\n",
    "            pass\n",
    "        else:\n",
    "            l6.append(i)\n",
    "\n",
    "    return  l6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ffae2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_des_tours_de_paroles_par = []\n",
    "for convers in liste_de_conversations_diarisees:\n",
    "    str_conversation=str(convers)\n",
    "    str_conversation = str_conversation.replace('[pers.pre]', '[pers pre]')   #supprimer des NER\n",
    "    str_conversation = str_conversation.replace('[pers.nom]', '[pers nom]')\n",
    "    str_conversation = str_conversation.replace('[pers.nom.epl]', '[pers nom epl]')\n",
    "    str_conversation = str_conversation.replace('[num tel]', '[num tel]')\n",
    "    str_conversation = str_conversation.replace('[loc.voie]', '[loc voie]')\n",
    "    str_conversation = str_conversation.replace('[loc.cp]', '[loc cp]')\n",
    "    str_conversation_vf = str_conversation.replace('[pers.mail]', '[pers mail]')\n",
    "    conversation_intermediaire = str_conversation_vf.split(\".\")\n",
    "    liste_des_tours_de_paroles_par.append(conversation_intermediaire)\n",
    "#print(len(liste_des_tours_de_paroles_par[0]))\n",
    "#print(liste_des_tours_de_paroles_par)\n",
    "print(len(liste_des_tours_de_paroles_par), 'conservations')#récupère la liste des conversations entières\n",
    "\n",
    "liste_des_tours = []\n",
    "\n",
    "for conver in liste_des_tours_de_paroles_par:\n",
    "    a = nettoie_resume(conver)\n",
    "    liste_des_tours.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d117f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_conv = latest_transcription_df['nom']\n",
    "#créer un dataframe pour output\n",
    "new_df = pd.DataFrame(columns = ['nom','synthese'])\n",
    "new_df['nom'] = id_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07aa76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resume_all (liste):\n",
    "    nlp = spacy.load('fr_core_news_md')\n",
    "\n",
    "    matrice_des_conversations_tokenizes = []\n",
    "    matrice_des_conversations_vectorizes = []\n",
    "    compteur = 0\n",
    "    for conversation in liste :\n",
    "        compteur+=1\n",
    "        if len(liste)%compteur == 0:\n",
    "            print(\"en cours...\")\n",
    "        array_des_tours_de_paroles_vectorizes = []\n",
    "        array_des_tours_de_paroles_tokenizes = []\n",
    "        for tour_de_parole in conversation:\n",
    "            tour_de_parole_vectorize = []\n",
    "            tour_de_parole_tokenize =[]\n",
    "            tour_de_parole_spacy = nlp(tour_de_parole)\n",
    "            for token in tour_de_parole_spacy:\n",
    "                tour_de_parole_vectorize.append(token.vector_norm if token.has_vector == True else 0)#padding\n",
    "                tour_de_parole_tokenize.append(token)\n",
    "            array_des_tours_de_paroles_vectorizes.append(tour_de_parole_vectorize)\n",
    "            array_des_tours_de_paroles_tokenizes.append(tour_de_parole_tokenize)\n",
    "        matrice_des_conversations_vectorizes.append(array_des_tours_de_paroles_vectorizes)\n",
    "        matrice_des_conversations_tokenizes.append(array_des_tours_de_paroles_tokenizes)\n",
    "    len(matrice_des_conversations_vectorizes) == len(matrice_des_conversations_tokenizes)\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.metrics import pairwise_distances_argmin_min\n",
    "    list_summary = []\n",
    "    for i in range(len(np.array(matrice_des_conversations_vectorizes))):\n",
    "        doc_vec = []\n",
    "        document = np.array(matrice_des_conversations_vectorizes[i])\n",
    "\n",
    "\n",
    "        for tdp in document:\n",
    "\n",
    "            tdp_vec = np.zeros((100))\n",
    "            for vk in tdp:\n",
    "\n",
    "                tdp_vec = np.append(tdp_vec,vk)  #ajouter les valeurs de document\n",
    "                tdp_vec = np.delete(tdp_vec, 0)  #supprimer 0\n",
    "\n",
    "            doc_vec.append(tdp_vec)\n",
    "\n",
    "        if len(doc_vec) >= 25:\n",
    "\n",
    "            n_clusters = 25\n",
    "\n",
    "            kmeans = KMeans(n_clusters=n_clusters)\n",
    "            means = kmeans.fit(doc_vec)\n",
    "            avg = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            docu = liste[i]\n",
    "            #print(doc)\n",
    "\n",
    "\n",
    "\n",
    "            for j in range(n_clusters):\n",
    "                idx = np.where(kmeans.labels_ == j)[0]\n",
    "                avg.append(np.mean(idx))\n",
    "            #print(avg)\n",
    "\n",
    "\n",
    "            closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, doc_vec)\n",
    "            ordering = sorted(range(n_clusters), key=lambda k: avg[k])\n",
    "\n",
    "            #print(ordering)\n",
    "            liste_cl=[]\n",
    "            for p in ordering:\n",
    "                cl = closest[p]\n",
    "                liste_cl.append(cl)\n",
    "            liste_cl = sorted(liste_cl)\n",
    "            #print(liste_cl)\n",
    "            #print(document[10])\n",
    "\n",
    "\n",
    "            summary = '//'.join(docu[idx] for idx in liste_cl)\n",
    "\n",
    "            list_summary.append(summary)\n",
    "\n",
    "        else:\n",
    "            list_summary.append(liste[i])\n",
    "\n",
    "    return list_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e362b895",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['synthese'] = resume_all(liste_des_tours)\n",
    "synthese_transcription = dataiku.Dataset(\"synthese_transcription\")\n",
    "synthese_transcription.write_with_schema(new_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
